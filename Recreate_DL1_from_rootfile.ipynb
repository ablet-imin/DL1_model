{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `uproot` can read ROOT objects from root type file without relying on ROOT I/O library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "#from keras.layers.core import MaxoutDense\n",
    "\n",
    "from models.maxout_layers import Maxout1D\n",
    "from models.normalization_layer import Normalization\n",
    "\n",
    "\n",
    "def get_net_struct(obj_path):\n",
    "    '''\n",
    "    Directly read ROOT objects specified in obj_path.\n",
    "    obj_path -- file.root:Tdiectory/subdirectory/..../obj\n",
    "    \n",
    "    In this fuction we only need TString obj from the file.\n",
    "    '''\n",
    "    with uproot.open(File_path) as net_config:\n",
    "    #convert string to dictionary\n",
    "        assert (type(net_config) is uproot.models.TObjString.Model_TObjString )\n",
    "        _struct = json.load(StringIO(net_config))\n",
    "    return _struct\n",
    "\n",
    "def print_dimention(weights):\n",
    "    for w in weights[1:]:\n",
    "        print(w[\"weights\"])\n",
    "\n",
    "def all_layers(weights):\n",
    "    for i, w in enumerate(weights):\n",
    "        print(i, w.keys())\n",
    "        \n",
    "def get_maxout_weights(NN_layer):\n",
    "    maxout_unit=0\n",
    "    maxout_h_unit=len(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "    in_features = len(NN_layer['sublayers'][maxout_unit]['weights'])//maxout_h_unit\n",
    "    maxout_weights=[]\n",
    "    maxout_biases = []\n",
    "    units = len(NN_layer['sublayers'])\n",
    "    \n",
    "    for maxout_unit in range(units):\n",
    "        maxout_weights.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['weights']\n",
    "                              ).reshape( maxout_h_unit, in_features).transpose() )\n",
    "        maxout_biases.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "                            )\n",
    "    \n",
    "    return (in_features, maxout_h_unit, units, \n",
    "            np.stack(maxout_weights, axis=2).reshape(in_features,maxout_h_unit*units),\n",
    "            np.stack(maxout_biases, axis=1).flatten() )\n",
    "   \n",
    "\n",
    "def get_dense_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    in_features = len(NN_layer['weights'])//h_unit\n",
    "    weight = np.array(NN_layer['weights']).reshape( h_unit, in_features).transpose()\n",
    "    return (in_features, h_unit, weight, np.array(NN_layer[\"bias\"]) )\n",
    "\n",
    "def get_BN_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    return (np.diag(np.array(NN_layer['weights'])),\n",
    "            np.array(NN_layer[\"bias\"]) )\n",
    "\n",
    "\n",
    "def pars_layers(layers):\n",
    "    N_layers = len(layers)\n",
    "    layersDic = {}\n",
    "    tf_layers = []\n",
    "    N_features = -1\n",
    "    for i, layer in enumerate(layers):\n",
    "        arch = layer[\"architecture\"]\n",
    "        if arch == 'maxout':\n",
    "            layer_name=\"maxout%s\"%i\n",
    "            \n",
    "            # return Nfeatures, hiden nodes, maxout units, weights, bias\n",
    "            v, h,unit, w, b = get_maxout_weights(layer) \n",
    "            if N_features<1:  N_features = v\n",
    "                \n",
    "            layersDic[layer_name] = [w, b]\n",
    "            tf_layers.append( Maxout1D(h, unit, name=layer_name) )\n",
    "            tf_layers.append( keras.layers.Activation(\n",
    "                                                        activation=layer[\"activation\"],\n",
    "                                                        name=\"activ%s\"%i \n",
    "                                                        )\n",
    "                            )\n",
    "            \n",
    "        elif arch == 'normalization':\n",
    "            layer_name=\"BN%s\"%i \n",
    "            layersDic[layer_name] = [*get_BN_weights(layer)]\n",
    "            tf_layers.append( Normalization(name=layer_name) )\n",
    "            \n",
    "        elif arch == 'dense':\n",
    "            layer_name=\"dense%s\"%i\n",
    "            #Ninputs, hiden nodes, weights, bias\n",
    "            v, h, w, b = get_dense_weights(layer)\n",
    "            if N_features<1: N_features = v\n",
    "            layersDic[layer_name]=[w, b ]\n",
    "            activation=\"relu\" if layer[\"activation\"]=='rectified' else layer[\"activation\"]\n",
    "            \n",
    "            tf_layers.append( keras.layers.Dense(h, activation=activation,\n",
    "                              kernel_initializer='glorot_uniform', name=layer_name)\n",
    "                            )\n",
    "        else:\n",
    "            raise Exception('Unkown layer %s'%arch )\n",
    "            \n",
    "    return N_features, tf_layers, layersDic\n",
    "\n",
    "#create NN from input layers\n",
    "#each layer has unique name\n",
    "def get_DL1(N_features, dl1_layers, lr=0.005, drops=None):\n",
    "    \n",
    "    In = tf.keras.layers.Input(shape=(N_features,), name=\"input\")\n",
    "    x = In\n",
    "    drop_index=0\n",
    "    for layer in dl1_layers[:-1]:\n",
    "        if drops:\n",
    "            if 'BN' in layer.name:\n",
    "                x = keras.layers.Dropout( drops[drop_index], \n",
    "                                          name=\"drop%s\"%drop_index )(x, training=True)\n",
    "                drop_index=drop_index+1\n",
    "        x = layer(x) \n",
    "        \n",
    "    predictions = dl1_layers[-1](x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=In, outputs=predictions)\n",
    "    model_optimizer = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=model_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#uproot.open(\"BTagCalibRUN2-08-40.root\").keys()\n",
    "\n",
    "Interested Dl1 networkes\n",
    "* 'DL1',\n",
    "* 'DL1/AntiKt4EMTopo',\n",
    "* 'DL1/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1mu',\n",
    "* 'DL1mu/AntiKt4EMTopo',\n",
    "* 'DL1mu/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1rnn',\n",
    "* 'DL1rnn/AntiKt4EMTopo',\n",
    "* 'DL1rnn/AntiKt4EMTopo/net_configuration',\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL1   \n",
    "DL1 is a neural network trained by b-tagging group. \n",
    "Model weights are stored in `BTagCalibRUN2-08-40.root` file as a string object. Our goal is to read the weight strings and convert them into json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 17\n",
      "0 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "1 dict_keys(['bias', 'weights', 'architecture'])\n",
      "2 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "3 dict_keys(['bias', 'weights', 'architecture'])\n",
      "4 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "5 dict_keys(['bias', 'weights', 'architecture'])\n",
      "6 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "7 dict_keys(['bias', 'weights', 'architecture'])\n",
      "8 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "9 dict_keys(['bias', 'weights', 'architecture'])\n",
      "10 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "11 dict_keys(['bias', 'weights', 'architecture'])\n",
      "12 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "13 dict_keys(['bias', 'weights', 'architecture'])\n",
      "14 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "15 dict_keys(['bias', 'weights', 'architecture'])\n",
      "16 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n"
     ]
    }
   ],
   "source": [
    "#filename.root:Tdirectory/directory/obj\n",
    "File_path=\"BTagCalibRUN2-08-40.root:DL1/AntiKt4EMTopo/net_configuration\"\n",
    "\n",
    "DL1_struct = get_net_struct(File_path)\n",
    "DL1_weights = DL1_struct['layers']\n",
    "\n",
    "print(f\"number of layers: {len(DL1_weights)}\")\n",
    "all_layers(DL1_weights)\n",
    "#print_dimention(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sublayers` stored weights and biases of MaxoutDense layers.  Total of two MaxoutDense leyers are stored.  \n",
    "Other layers are BatchNoramlization and Dense layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rectified'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check maxout layers\n",
    "sublayers = DL1_struct['layers'][0]['sublayers']\n",
    "\n",
    "#output size of a maxout layer\n",
    "print(len(sublayers[0]['bias']))\n",
    "\n",
    "#last Dense layer (output layer)\n",
    "DL1_struct['layers'][-5]['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rectified'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DL1_struct['layers'][-3]['activation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bellow, I defin DL1rnn with tensorflow keras API. Instead of train the new network, I will set weights of each layer to the weights extracted above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "maxout0 (Maxout1D)           (None, 72)                75600     \n",
      "_________________________________________________________________\n",
      "activ0 (Activation)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "BN1 (Normalization)          (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 57)                4161      \n",
      "_________________________________________________________________\n",
      "BN3 (Normalization)          (None, 57)                3306      \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "BN5 (Normalization)          (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 48)                2928      \n",
      "_________________________________________________________________\n",
      "BN7 (Normalization)          (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "BN9 (Normalization)          (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "maxout10 (Maxout1D)          (None, 24)                22200     \n",
      "_________________________________________________________________\n",
      "activ10 (Activation)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "BN11 (Normalization)         (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense12 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "BN13 (Normalization)         (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense14 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "BN15 (Normalization)         (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 127,236\n",
      "Trainable params: 127,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "maxout0\n",
      "BN1\n",
      "dense2\n",
      "BN3\n",
      "dense4\n",
      "BN5\n",
      "dense6\n",
      "BN7\n",
      "dense8\n",
      "BN9\n",
      "maxout10\n",
      "BN11\n",
      "dense12\n",
      "BN13\n",
      "dense14\n",
      "BN15\n",
      "dense16\n"
     ]
    }
   ],
   "source": [
    "#DL1_layers = [ 72, 57, 60, 48, 36,24, 12, 6]\n",
    "DL1_dropouts = [0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "dropout_enable = False\n",
    "\n",
    "#findout input size, and weight matrix for each layer.\n",
    "#create corresponding tensorflow layers and store them in a list.\n",
    "features, dl1_layers, dl1_weights = pars_layers(DL1_struct['layers'])\n",
    "\n",
    "DL1_model = get_DL1(features , dl1_layers, drops=DL1_dropouts if dropout_enable else None )\n",
    "DL1_model.summary()\n",
    "\n",
    "def set_dl1_weights(model, weights):\n",
    "    for name in weights.keys():\n",
    "        print(name)\n",
    "        layer = model.get_layer( name=name)\n",
    "        layer.set_weights(weights[name])\n",
    "        \n",
    "set_dl1_weights(model=DL1_model, weights=dl1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model with a random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'BN1/norm_w:0' shape=(72, 72) dtype=float32, numpy=\n",
       " array([[0.6246186 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.60878783, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.32325083, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.6890345 , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.75887954,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.6151385 ]], dtype=float32)>,\n",
       " <tf.Variable 'BN1/norm_b:0' shape=(72,) dtype=float32, numpy=\n",
       " array([-3.0784302e+00, -4.3211836e-01, -1.0646136e+00, -3.4751779e-01,\n",
       "        -1.2186527e-01, -1.0378135e+00, -1.1474031e-01, -5.0850809e-03,\n",
       "        -3.2649010e-02, -3.1456509e-01, -9.0102834e-01,  7.6231003e-02,\n",
       "         8.1319571e-02, -4.6990597e-01, -3.8315630e-01,  3.1032479e-01,\n",
       "        -3.9813936e-02, -5.2276640e+00, -1.4079063e-01, -9.9650741e-02,\n",
       "        -4.6313763e-01,  2.3944244e+00, -6.1936444e-01,  9.7934556e-01,\n",
       "        -5.7456625e-01,  6.8463087e-02,  7.8013682e-01, -3.7704542e-01,\n",
       "        -2.7581153e+00, -1.7565046e+01, -2.7117211e-01, -1.2390092e-01,\n",
       "        -9.0235871e-01, -2.4091482e-02,  1.3709307e-01, -1.1386449e+00,\n",
       "        -1.2258375e-01, -3.6463554e+00, -3.8207108e-01, -3.0194509e-01,\n",
       "         7.1528852e-02, -8.7348342e-01, -1.2361243e-01, -2.7500933e-01,\n",
       "        -3.9311337e-01, -5.1723790e+00,  4.7019815e-01,  2.7791396e-01,\n",
       "        -5.5217272e-01,  1.1531309e+00, -1.4228380e+00, -6.7379105e-01,\n",
       "        -9.7406971e-01, -4.0237707e-01, -5.7246351e+00, -4.0217417e-01,\n",
       "        -3.9096785e-01, -8.0624092e-01, -1.0305122e+01, -6.7164505e-01,\n",
       "         1.1043024e-01, -3.7047279e+00,  4.0672541e-02, -2.0061111e+01,\n",
       "        -4.5338964e+00, -5.2189696e-01, -6.7532659e-03, -1.4686127e+00,\n",
       "        -3.4961677e+00, -3.5492880e+00, -2.3454422e-01, -3.3053288e-01],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test model with dummy inputs\n",
    "DL1_model(inputs=np.random.random((5, features)), training=False)\n",
    "DL1_model.layers[3].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save this model\n",
    "\n",
    "This model contains a custom layer which can not be saved as a single `.h5` file with `save(\"model.h5\")`. Becuase, the custom layer implemented in the model is not know, and you will get an error when loading the model again.   \n",
    "Alternatively, `save(\"DL1_AntiKt4EMTopo\")` will save our model into a directory which contains model architecture and weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL1_AntiKt4EMTopo/assets\n"
     ]
    }
   ],
   "source": [
    "model_file = \"DL1_AntiKt4EMTopo_dropout\" if dropout_enable else \"DL1_AntiKt4EMTopo\"\n",
    "DL1_model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "load_model() fuction `tf.keras.models.load_model(\"DL1_AntiKt4EMTopo\")` can directly load model architectures and weights including the custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'mean:0' shape=(12,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(24,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'variance:0' shape=(57,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'variance:0' shape=(72,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(24,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(48,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(6,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'variance:0' shape=(12,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(60,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'mean:0' shape=(60,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(57,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(36,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(72,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(48,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(36,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>, <tf.Variable 'variance:0' shape=(6,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1.], dtype=float32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d806bfc9441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m   \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m    890\u001b[0m                             ckpt_options, filters)\n\u001b[1;32m    891\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36massert_existing_objects_matched\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m             self._checkpoint.object_by_proto_id.values()))\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munused_python_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m       raise AssertionError(\n\u001b[0m\u001b[1;32m    806\u001b[0m           (\"Some Python objects were not bound to checkpointed values, likely \"\n\u001b[1;32m    807\u001b[0m            \"due to changes in the Python program: %s\") %\n",
      "\u001b[0;31mAssertionError\u001b[0m: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'mean:0' shape=(12,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(24,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'variance:0' shape=(57,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'variance:0' shape=(72,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(24,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(48,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(6,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'variance:0' shape=(12,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(60,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>, <tf.Variable 'mean:0' shape=(60,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(57,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'mean:0' shape=(36,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(72,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'mean:0' shape=(48,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>, <tf.Variable 'count:0' shape=() dtype=int64, numpy=0>, <tf.Variable 'variance:0' shape=(36,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>, <tf.Variable 'variance:0' shape=(6,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1.], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "test_model = tf.keras.models.load_model(model_file)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(inputs=np.random.random((5, features)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL1_weights[0]['sublayers'][0]['weights'][:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl1_weights['maxout0'][0][:,0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "b = tf.constant([7, 9, 11], shape=[3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.matvec(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
