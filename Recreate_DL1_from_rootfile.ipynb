{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `uproot` can read ROOT objects from root type file without relying on ROOT I/O library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "#from keras.layers.core import MaxoutDense\n",
    "\n",
    "from models.maxout_layers import Maxout1D\n",
    "\n",
    "\n",
    "def get_net_struct(obj_path):\n",
    "    '''\n",
    "    Directly read ROOT objects specified in obj_path.\n",
    "    obj_path -- file.root:Tdiectory/subdirectory/..../obj\n",
    "    \n",
    "    In this fuction we only need TString obj from the file.\n",
    "    '''\n",
    "    with uproot.open(File_path) as net_config:\n",
    "    #convert string to dictionary\n",
    "        assert (type(net_config) is uproot.models.TObjString.Model_TObjString )\n",
    "        _struct = json.load(StringIO(net_config))\n",
    "    return _struct\n",
    "\n",
    "def print_dimention(weights):\n",
    "    for w in weights[1:]:\n",
    "        print(w[\"weights\"])\n",
    "\n",
    "def all_layers(weights):\n",
    "    for i, w in enumerate(weights):\n",
    "        print(i, w.keys())\n",
    "        \n",
    "def get_maxout_weights(NN_layer):\n",
    "    maxout_unit=0\n",
    "    maxout_h_unit=len(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "    in_features = len(NN_layer['sublayers'][maxout_unit]['weights'])//maxout_h_unit\n",
    "    weight = np.array(NN_layer['sublayers'][maxout_unit]['weights']).reshape(in_features, maxout_h_unit)\n",
    "    maxout_weights=[]\n",
    "    maxout_biases = []\n",
    "    units = len(NN_layer['sublayers'])\n",
    "    for maxout_unit in range(units):\n",
    "        maxout_weights.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['weights']\n",
    "                              ).reshape( maxout_h_unit, in_features).transpose() )\n",
    "        maxout_biases.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "                            )\n",
    "    #return (in_features, maxout_h_unit, units, np.concatenate(maxout_weights, axis=1),\n",
    "            #np.array( maxout_biases).flatten())\n",
    "    return (in_features, maxout_h_unit, units, \n",
    "            np.stack(maxout_weights, axis=2).reshape(in_features,maxout_h_unit*units),\n",
    "            np.stack(maxout_biases, axis=1).flatten() )\n",
    "   \n",
    "\n",
    "def get_dense_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    in_features = len(NN_layer['weights'])//h_unit\n",
    "    weight = np.array(NN_layer['weights']).reshape( h_unit, in_features).transpose()\n",
    "    return (in_features, h_unit, weight, np.array(NN_layer[\"bias\"]) )\n",
    "\n",
    "def get_BN_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    return (np.array(NN_layer['weights']),\n",
    "            np.array(NN_layer[\"bias\"]), \n",
    "            np.array(h_unit*[0]), np.array(h_unit*[1]) )\n",
    "\n",
    "def pars_layers(layers):\n",
    "    N_layers = len(layers)\n",
    "    layersDic = {}\n",
    "    tf_layers = []\n",
    "    N_features = -1\n",
    "    for i, layer in enumerate(layers):\n",
    "        arch = layer[\"architecture\"]\n",
    "        if arch == 'maxout':\n",
    "            layer_name=\"maxout%s\"%i\n",
    "            v, h,unit, w, b = get_maxout_weights(layer)\n",
    "            if N_features<1:\n",
    "                N_features = v\n",
    "            layersDic[layer_name]=[w, b]\n",
    "            tf_layers.append(Maxout1D(h, unit, name=layer_name))\n",
    "            activation = layer[\"activation\"]\n",
    "            tf_layers.append( keras.layers.Activation(activation=activation, name=\"activ%s\"%i ) )\n",
    "        elif arch == 'normalization':\n",
    "            layer_name=\"BN%s\"%i \n",
    "            layersDic[layer_name]=[*get_BN_weights(layer)]\n",
    "            tf_layers.append(keras.layers.BatchNormalization(name=layer_name))\n",
    "            \n",
    "        elif arch == 'dense':\n",
    "            layer_name=\"dense%s\"%i\n",
    "            v, h, w, b = get_dense_weights(layer)\n",
    "            if N_features<1:\n",
    "                N_features = v\n",
    "            layersDic[layer_name]=[w, b ]\n",
    "            activation=\"relu\" \n",
    "            if layer[\"activation\"]=='softmax': activation=\"softmax\"\n",
    "            tf_layers.append( keras.layers.Dense(h, activation=activation,\n",
    "                  kernel_initializer='glorot_uniform', name=layer_name) )\n",
    "            #tf_layers.append( keras.layers.Activation(activation='elu', name=\"activ%s\"%i ) )\n",
    "        else:\n",
    "            raise Exception('Unkown layer %s'%arch )\n",
    "    return N_features, tf_layers, layersDic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#uproot.open(\"BTagCalibRUN2-08-40.root\").keys()\n",
    "\n",
    "Interested Dl1 networkes\n",
    "* 'DL1',\n",
    "* 'DL1/AntiKt4EMTopo',\n",
    "* 'DL1/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1mu',\n",
    "* 'DL1mu/AntiKt4EMTopo',\n",
    "* 'DL1mu/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1rnn',\n",
    "* 'DL1rnn/AntiKt4EMTopo',\n",
    "* 'DL1rnn/AntiKt4EMTopo/net_configuration',\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL1rnn \n",
    "DL1rnn is a neural network trained by b-tagging group. \n",
    "Model weights are stored in `BTagCalibRUN2-08-40.root` file as a string object. Our goal is to read the weight strings and convert them into json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 17\n",
      "0 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "1 dict_keys(['bias', 'weights', 'architecture'])\n",
      "2 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "3 dict_keys(['bias', 'weights', 'architecture'])\n",
      "4 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "5 dict_keys(['bias', 'weights', 'architecture'])\n",
      "6 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "7 dict_keys(['bias', 'weights', 'architecture'])\n",
      "8 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "9 dict_keys(['bias', 'weights', 'architecture'])\n",
      "10 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "11 dict_keys(['bias', 'weights', 'architecture'])\n",
      "12 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "13 dict_keys(['bias', 'weights', 'architecture'])\n",
      "14 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "15 dict_keys(['bias', 'weights', 'architecture'])\n",
      "16 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n"
     ]
    }
   ],
   "source": [
    "#filename.root:Tdirectory/directory/obj\n",
    "File_path=\"BTagCalibRUN2-08-40.root:DL1/AntiKt4EMTopo/net_configuration\"\n",
    "\n",
    "DL1_struct = get_net_struct(File_path)\n",
    "DL1_weights = DL1_struct['layers']\n",
    "\n",
    "print(f\"number of layers: {len(DL1_weights)}\")\n",
    "all_layers(DL1_weights)\n",
    "#print_dimention(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sublayers` stored weights and biases of MaxoutDense layers.  Total of two MaxoutDense leyers are stored.  \n",
    "Other layers are BatchNoramlization and Dense layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sublayers = DL1_struct['layers'][0]['sublayers']\n",
    "print(len(sublayers[0]['bias']))\n",
    "DL1_struct['layers'][-1]['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DL1_struct['layers'][-3]['activation']\n",
    "features, dl1_layers, dl1_weights = pars_layers(DL1_struct['layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f8937e03dc0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl1_layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bellow, I defin DL1rnn with tensorflow keras API. Instead of train the new network, I will set weights of each layer to the weights extracted above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DL1_layers = [ 72, 57, 60, 48, 36,24, 12, 6]\n",
    "DL1_dropouts = [0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "dropout_enable = False\n",
    "\n",
    "# DL1rnn definition\n",
    "#maxoutdense1 = MaxoutDense1D(72, 25)\n",
    "#maxoutdense6 = MaxoutDense1D(24, 25)\n",
    "def get_DL1(N_features, dl1_layers, lr=0.005, drops=None):\n",
    "    \n",
    "    In = tf.keras.layers.Input(shape=(N_features,), name=\"input\")\n",
    "    x = In\n",
    "    drop_index=0\n",
    "    for layer in dl1_layers[:-1]:\n",
    "        if drops:\n",
    "            if 'BN' in layer.name:\n",
    "                x = keras.layers.Dropout(drops[drop_index], name=\"drop%s\"%drop_index)(x, training=True)\n",
    "                drop_index=drop_index+1\n",
    "        x = layer(x) \n",
    "        \n",
    "    predictions = dl1_layers[-1](x)\n",
    "\n",
    "    model = keras.models.Model(inputs=In, outputs=predictions)\n",
    "\n",
    "    model_optimizer = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=model_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "maxout0 (Maxout1D)           (None, 72)                75600     \n",
      "_________________________________________________________________\n",
      "activ0 (Activation)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 72)                288       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 57)                4161      \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 57)                228       \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 48)                2928      \n",
      "_________________________________________________________________\n",
      "BN7 (BatchNormalization)     (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "BN9 (BatchNormalization)     (None, 36)                144       \n",
      "_________________________________________________________________\n",
      "maxout10 (Maxout1D)          (None, 24)                22200     \n",
      "_________________________________________________________________\n",
      "activ10 (Activation)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "BN11 (BatchNormalization)    (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dense12 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "BN13 (BatchNormalization)    (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense14 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "BN15 (BatchNormalization)    (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 111,792\n",
      "Trainable params: 111,162\n",
      "Non-trainable params: 630\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DL1_model = get_DL1(41,dl1_layers, drops=DL1_dropouts if dropout_enable else None )\n",
    "DL1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxout0\n",
      "BN1\n",
      "dense2\n",
      "BN3\n",
      "dense4\n",
      "BN5\n",
      "dense6\n",
      "BN7\n",
      "dense8\n",
      "BN9\n",
      "maxout10\n",
      "BN11\n",
      "dense12\n",
      "BN13\n",
      "dense14\n",
      "BN15\n",
      "dense16\n"
     ]
    }
   ],
   "source": [
    "def set_dl1_weights(model, weights):\n",
    "    for name in weights.keys():\n",
    "        print(name)\n",
    "        layer = model.get_layer( name=name)\n",
    "        layer.set_weights(weights[name])\n",
    "        \n",
    "set_dl1_weights(model=DL1_model, weights=dl1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[1.1024110e-04, 6.9891721e-02, 9.2999798e-01],\n",
       "       [3.3516635e-03, 1.4459714e-01, 8.5205126e-01],\n",
       "       [8.9207688e-06, 3.9894938e-02, 9.6009618e-01],\n",
       "       [3.4105282e-03, 1.4511083e-01, 8.5147870e-01],\n",
       "       [5.2438336e-03, 1.5822791e-01, 8.3652824e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test model with dummy inputs\n",
    "DL1_model(inputs=np.random.random((5, 41)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL1_AntiKt4EMTopo/assets\n"
     ]
    }
   ],
   "source": [
    "model_file = \"DL1_AntiKt4EMTopo_dropout\" if dropout_enable else \"DL1_AntiKt4EMTopo\"\n",
    "DL1_model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save this model\n",
    "\n",
    "This model contains a custom layer which can not be saved as a single `.h5` file with `save(\"model.h5\")`. Becuase, the custom layer implemented in the model is not know, and you will get an error when loading the model again.   \n",
    "Alternatively, `save(\"DL1_AntiKt4EMTopo\")` will save our model into a directory which contains model architecture and weights.\n",
    "\n",
    "## Load model\n",
    "\n",
    "load_model() fuction `tf.keras.models.load_model(\"DL1_AntiKt4EMTopo\")` can directly load model architectures and weights including the custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "maxout0 (Maxout1D)           (None, 72)                75600     \n",
      "_________________________________________________________________\n",
      "activ0 (Activation)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 72)                288       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 57)                4161      \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 57)                228       \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 48)                2928      \n",
      "_________________________________________________________________\n",
      "BN7 (BatchNormalization)     (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "BN9 (BatchNormalization)     (None, 36)                144       \n",
      "_________________________________________________________________\n",
      "maxout10 (Maxout1D)          (None, 24)                22200     \n",
      "_________________________________________________________________\n",
      "activ10 (Activation)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "BN11 (BatchNormalization)    (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dense12 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "BN13 (BatchNormalization)    (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense14 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "BN15 (BatchNormalization)    (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 111,792\n",
      "Trainable params: 111,162\n",
      "Non-trainable params: 630\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = tf.keras.models.load_model(model_file)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f893a184040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f895001cb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[8.7461853e-01, 6.7300670e-02, 5.8080815e-02],\n",
       "       [9.3420306e-03, 1.7718722e-01, 8.1347072e-01],\n",
       "       [1.2676546e-04, 7.2026722e-02, 9.2784649e-01],\n",
       "       [5.5192335e-04, 9.9129550e-02, 9.0031850e-01],\n",
       "       [1.2056185e-05, 4.2716261e-02, 9.5727170e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(inputs=np.random.random((5, 41)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
