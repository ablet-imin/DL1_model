{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `uproot` can read ROOT objects from root type file without relying on ROOT I/O library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "#from keras.layers.core import MaxoutDense\n",
    "\n",
    "from models.maxout_layers import Maxout1D\n",
    "\n",
    "\n",
    "def get_net_struct(obj_path):\n",
    "    '''\n",
    "    Directly read ROOT objects specified in obj_path.\n",
    "    obj_path -- file.root:Tdiectory/subdirectory/..../obj\n",
    "    \n",
    "    In this fuction we only need TString obj from the file.\n",
    "    '''\n",
    "    with uproot.open(File_path) as net_config:\n",
    "    #convert string to dictionary\n",
    "        assert (type(net_config) is uproot.models.TObjString.Model_TObjString )\n",
    "        _struct = json.load(StringIO(net_config))\n",
    "    return _struct\n",
    "\n",
    "def print_dimention(weights):\n",
    "    for w in weights[1:]:\n",
    "        print(w[\"weights\"])\n",
    "\n",
    "def all_layers(weights):\n",
    "    for i, w in enumerate(weights):\n",
    "        print(i, w.keys())\n",
    "        \n",
    "def get_maxout_weights(NN_layer):\n",
    "    maxout_unit=0\n",
    "    maxout_h_unit=len(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "    in_features = len(NN_layer['sublayers'][maxout_unit]['weights'])//maxout_h_unit\n",
    "    maxout_weights=[]\n",
    "    maxout_biases = []\n",
    "    units = len(NN_layer['sublayers'])\n",
    "    \n",
    "    for maxout_unit in range(units):\n",
    "        maxout_weights.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['weights']\n",
    "                              ).reshape( maxout_h_unit, in_features).transpose() )\n",
    "        maxout_biases.append(\n",
    "                                np.array(NN_layer['sublayers'][maxout_unit]['bias'])\n",
    "                            )\n",
    "    \n",
    "    return (in_features, maxout_h_unit, units, \n",
    "            np.stack(maxout_weights, axis=2).reshape(in_features,maxout_h_unit*units),\n",
    "            np.stack(maxout_biases, axis=1).flatten() )\n",
    "   \n",
    "\n",
    "def get_dense_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    in_features = len(NN_layer['weights'])//h_unit\n",
    "    weight = np.array(NN_layer['weights']).reshape( h_unit, in_features).transpose()\n",
    "    return (in_features, h_unit, weight, np.array(NN_layer[\"bias\"]) )\n",
    "\n",
    "def get_BN_weights(NN_layer):\n",
    "    h_unit=len(NN_layer[\"bias\"])\n",
    "    return (np.array(NN_layer['weights']),\n",
    "            np.array(NN_layer[\"bias\"]), \n",
    "            np.array(h_unit*[0]), np.array(h_unit*[1]) )\n",
    "\n",
    "\n",
    "def pars_layers(layers):\n",
    "    N_layers = len(layers)\n",
    "    layersDic = {}\n",
    "    tf_layers = []\n",
    "    N_features = -1\n",
    "    for i, layer in enumerate(layers):\n",
    "        arch = layer[\"architecture\"]\n",
    "        if arch == 'maxout':\n",
    "            layer_name=\"maxout%s\"%i\n",
    "            \n",
    "            # return Nfeatures, hiden nodes, maxout units, weights, bias\n",
    "            v, h,unit, w, b = get_maxout_weights(layer) \n",
    "            if N_features<1:  N_features = v\n",
    "                \n",
    "            layersDic[layer_name] = [w, b]\n",
    "            tf_layers.append( Maxout1D(h, unit, name=layer_name) )\n",
    "            tf_layers.append( keras.layers.Activation(\n",
    "                                                        activation=layer[\"activation\"],\n",
    "                                                        name=\"activ%s\"%i \n",
    "                                                        )\n",
    "                            )\n",
    "            \n",
    "        elif arch == 'normalization':\n",
    "            layer_name=\"BN%s\"%i \n",
    "            layersDic[layer_name] = [*get_BN_weights(layer)]\n",
    "            tf_layers.append( keras.layers.BatchNormalization(name=layer_name) )\n",
    "            \n",
    "        elif arch == 'dense':\n",
    "            layer_name=\"dense%s\"%i\n",
    "            #Ninputs, hiden nodes, weights, bias\n",
    "            v, h, w, b = get_dense_weights(layer)\n",
    "            if N_features<1: N_features = v\n",
    "            layersDic[layer_name]=[w, b ]\n",
    "            activation=\"softmax\" if layer[\"activation\"]=='softmax' else \"relu\"\n",
    "            \n",
    "            tf_layers.append( keras.layers.Dense(h, activation=activation,\n",
    "                              kernel_initializer='glorot_uniform', name=layer_name)\n",
    "                            )\n",
    "        else:\n",
    "            raise Exception('Unkown layer %s'%arch )\n",
    "            \n",
    "    return N_features, tf_layers, layersDic\n",
    "\n",
    "#create NN from input layers\n",
    "#each layer has unique name\n",
    "def get_DL1(N_features, dl1_layers, lr=0.005, drops=None):\n",
    "    \n",
    "    In = tf.keras.layers.Input(shape=(N_features,), name=\"input\")\n",
    "    x = In\n",
    "    drop_index=0\n",
    "    for layer in dl1_layers[:-1]:\n",
    "        if drops:\n",
    "            if 'BN' in layer.name:\n",
    "                x = keras.layers.Dropout( drops[drop_index], \n",
    "                                          name=\"drop%s\"%drop_index )(x, training=True)\n",
    "                drop_index=drop_index+1\n",
    "        x = layer(x) \n",
    "        \n",
    "    predictions = dl1_layers[-1](x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=In, outputs=predictions)\n",
    "    model_optimizer = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=model_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#uproot.open(\"BTagCalibRUN2-08-40.root\").keys()\n",
    "\n",
    "Interested Dl1 networkes\n",
    "* 'DL1',\n",
    "* 'DL1/AntiKt4EMTopo',\n",
    "* 'DL1/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1mu',\n",
    "* 'DL1mu/AntiKt4EMTopo',\n",
    "* 'DL1mu/AntiKt4EMTopo/net_configuration',\n",
    "* 'DL1rnn',\n",
    "* 'DL1rnn/AntiKt4EMTopo',\n",
    "* 'DL1rnn/AntiKt4EMTopo/net_configuration',\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL1   \n",
    "DL1 is a neural network trained by b-tagging group. \n",
    "Model weights are stored in `BTagCalibRUN2-08-40.root` file as a string object. Our goal is to read the weight strings and convert them into json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 17\n",
      "0 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "1 dict_keys(['bias', 'weights', 'architecture'])\n",
      "2 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "3 dict_keys(['bias', 'weights', 'architecture'])\n",
      "4 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "5 dict_keys(['bias', 'weights', 'architecture'])\n",
      "6 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "7 dict_keys(['bias', 'weights', 'architecture'])\n",
      "8 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "9 dict_keys(['bias', 'weights', 'architecture'])\n",
      "10 dict_keys(['sublayers', 'activation', 'architecture'])\n",
      "11 dict_keys(['bias', 'weights', 'architecture'])\n",
      "12 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "13 dict_keys(['bias', 'weights', 'architecture'])\n",
      "14 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n",
      "15 dict_keys(['bias', 'weights', 'architecture'])\n",
      "16 dict_keys(['bias', 'weights', 'activation', 'architecture'])\n"
     ]
    }
   ],
   "source": [
    "#filename.root:Tdirectory/directory/obj\n",
    "File_path=\"BTagCalibRUN2-08-40.root:DL1/AntiKt4EMTopo/net_configuration\"\n",
    "\n",
    "DL1_struct = get_net_struct(File_path)\n",
    "DL1_weights = DL1_struct['layers']\n",
    "\n",
    "print(f\"number of layers: {len(DL1_weights)}\")\n",
    "all_layers(DL1_weights)\n",
    "#print_dimention(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sublayers` stored weights and biases of MaxoutDense layers.  Total of two MaxoutDense leyers are stored.  \n",
    "Other layers are BatchNoramlization and Dense layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check maxout layers\n",
    "sublayers = DL1_struct['layers'][0]['sublayers']\n",
    "\n",
    "#output size of a maxout layer\n",
    "print(len(sublayers[0]['bias']))\n",
    "\n",
    "#last Dense layer (output layer)\n",
    "DL1_struct['layers'][-1]['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rectified'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DL1_struct['layers'][-3]['activation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bellow, I defin DL1rnn with tensorflow keras API. Instead of train the new network, I will set weights of each layer to the weights extracted above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "maxout0 (Maxout1D)           (None, 72)                75600     \n",
      "_________________________________________________________________\n",
      "activ0 (Activation)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 72)                288       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 57)                4161      \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 57)                228       \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 48)                2928      \n",
      "_________________________________________________________________\n",
      "BN7 (BatchNormalization)     (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "BN9 (BatchNormalization)     (None, 36)                144       \n",
      "_________________________________________________________________\n",
      "maxout10 (Maxout1D)          (None, 24)                22200     \n",
      "_________________________________________________________________\n",
      "activ10 (Activation)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "BN11 (BatchNormalization)    (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dense12 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "BN13 (BatchNormalization)    (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense14 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "BN15 (BatchNormalization)    (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 111,792\n",
      "Trainable params: 111,162\n",
      "Non-trainable params: 630\n",
      "_________________________________________________________________\n",
      "maxout0\n",
      "BN1\n",
      "dense2\n",
      "BN3\n",
      "dense4\n",
      "BN5\n",
      "dense6\n",
      "BN7\n",
      "dense8\n",
      "BN9\n",
      "maxout10\n",
      "BN11\n",
      "dense12\n",
      "BN13\n",
      "dense14\n",
      "BN15\n",
      "dense16\n"
     ]
    }
   ],
   "source": [
    "#DL1_layers = [ 72, 57, 60, 48, 36,24, 12, 6]\n",
    "DL1_dropouts = [0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "dropout_enable = False\n",
    "\n",
    "#findout input size, and weight matrix for each layer.\n",
    "#create corresponding tensorflow layers and store them in a list.\n",
    "features, dl1_layers, dl1_weights = pars_layers(DL1_struct['layers'])\n",
    "\n",
    "DL1_model = get_DL1(features , dl1_layers, drops=DL1_dropouts if dropout_enable else None )\n",
    "DL1_model.summary()\n",
    "\n",
    "def set_dl1_weights(model, weights):\n",
    "    for name in weights.keys():\n",
    "        print(name)\n",
    "        layer = model.get_layer( name=name)\n",
    "        layer.set_weights(weights[name])\n",
    "        \n",
    "set_dl1_weights(model=DL1_model, weights=dl1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model with a random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[4.79424925e-04, 9.61689577e-02, 9.03351605e-01],\n",
       "       [4.87160549e-04, 9.65364203e-02, 9.02976453e-01],\n",
       "       [6.12047792e-04, 1.01391077e-01, 8.97996843e-01],\n",
       "       [5.96257858e-04, 1.00774914e-01, 8.98628891e-01],\n",
       "       [1.90150388e-06, 2.80996412e-02, 9.71898437e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test model with dummy inputs\n",
    "DL1_model(inputs=np.random.random((5, features)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save this model\n",
    "\n",
    "This model contains a custom layer which can not be saved as a single `.h5` file with `save(\"model.h5\")`. Becuase, the custom layer implemented in the model is not know, and you will get an error when loading the model again.   \n",
    "Alternatively, `save(\"DL1_AntiKt4EMTopo\")` will save our model into a directory which contains model architecture and weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL1_AntiKt4EMTopo/assets\n"
     ]
    }
   ],
   "source": [
    "model_file = \"DL1_AntiKt4EMTopo_dropout\" if dropout_enable else \"DL1_AntiKt4EMTopo\"\n",
    "DL1_model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "load_model() fuction `tf.keras.models.load_model(\"DL1_AntiKt4EMTopo\")` can directly load model architectures and weights including the custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 41)]              0         \n",
      "_________________________________________________________________\n",
      "maxout0 (Maxout1D)           (None, 72)                75600     \n",
      "_________________________________________________________________\n",
      "activ0 (Activation)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 72)                288       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 57)                4161      \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 57)                228       \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (None, 60)                3480      \n",
      "_________________________________________________________________\n",
      "BN5 (BatchNormalization)     (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense6 (Dense)               (None, 48)                2928      \n",
      "_________________________________________________________________\n",
      "BN7 (BatchNormalization)     (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dense8 (Dense)               (None, 36)                1764      \n",
      "_________________________________________________________________\n",
      "BN9 (BatchNormalization)     (None, 36)                144       \n",
      "_________________________________________________________________\n",
      "maxout10 (Maxout1D)          (None, 24)                22200     \n",
      "_________________________________________________________________\n",
      "activ10 (Activation)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "BN11 (BatchNormalization)    (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dense12 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "BN13 (BatchNormalization)    (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense14 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "BN15 (BatchNormalization)    (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 111,792\n",
      "Trainable params: 111,162\n",
      "Non-trainable params: 630\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = tf.keras.models.load_model(model_file)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[3.0696746e-03, 1.4203787e-01, 8.5489243e-01],\n",
       "       [5.2457904e-03, 1.5823771e-01, 8.3651644e-01],\n",
       "       [1.1101392e-03, 1.1500031e-01, 8.8388956e-01],\n",
       "       [3.0449126e-04, 8.7220304e-02, 9.1247517e-01],\n",
       "       [4.9223783e-03, 1.5629181e-01, 8.3878583e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(inputs=np.random.random((5, features)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
